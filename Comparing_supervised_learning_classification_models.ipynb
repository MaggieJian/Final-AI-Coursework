{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaggieJian/Final-AI-Coursework/blob/main/Comparing_supervised_learning_classification_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMi0zTh9y6WL"
      },
      "source": [
        "# Comparing supervised learning classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your drive"
      ],
      "metadata": {
        "id": "Sw5I10wyec5p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP5HiHG7hl-s",
        "outputId": "b88bd75b-07db-4f7c-a193-3e6aafc51f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-cnFsCNwF4p"
      },
      "source": [
        "## Loading the data\n",
        "From the previous notebook, you should have your training and testing data ready from this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt9iYcqDwLDp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "save_path = '/content/drive/MyDrive/Week_2/Week2_Sea-ice_and_Lead_Classification'\n",
        "X_train = np.load(os.path.join(save_path, 'X_train_balanced.npy'))\n",
        "X_test = np.load(os.path.join(save_path, 'X_test_balanced.npy'))\n",
        "y_train = np.load(os.path.join(save_path, 'y_train_balanced.npy'))\n",
        "y_test = np.load(os.path.join(save_path, 'y_test_balanced.npy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9DxCvwwIUCp"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)\n",
        "\n",
        "### Introduction to CNNs\n",
        "\n",
        "Convolutional Neural Networks, commonly known as CNNs, are a class of deep neural networks specially designed to process data with grid-like topology, such as images {cite}`Goodfellow-et-al-2016,lecun2015deep`. Originating from the visual cortex's biological processes, CNNs are revolutionising the way we understand and interpret visual data.\n",
        "\n",
        "### Why CNN for Image Data?\n",
        "\n",
        "Traditional neural networks, when used for images, suffer from two main issues:\n",
        "\n",
        "- **Too many parameters**: For a simple 256x256 colored image, an input layer would have \\(256 * 256 * 3 = 196,608\\) neurons, leading to an enormous number of parameters even in the first hidden layer.\n",
        "- **Loss of spatial information**: Flattening an image into a vector for traditional neural networks can lose the spatial hierarchies and patterns in the image, which are often crucial for understanding and interpreting visual data.\n",
        "\n",
        "CNNs address both issues by introducing convolutions.\n",
        "\n",
        "### Key Components of CNN\n",
        "\n",
        "1. **Convolutional Layer** {cite}`lecun2015deep`: This is the core building block of a CNN. It slides a filter (smaller in size than the input data) over the input data (like an image) to produce a feature map or convolved feature. The primary purpose of a convolution is to extract features from the input data.\n",
        "2. **Pooling Layer**: Pooling layers are used to reduce the dimensions of the feature maps, thereby reducing the number of parameters and computation in the network. The most common type of pooling is max pooling.\n",
        "3. **Fully Connected Layer**: After several convolutional and pooling layers, the final classification is done using one or more fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks.\n",
        "4. **Activation Functions**: Non-linearity is introduced into the CNN using activation functions. The Rectified Linear Unit (ReLU) is the most commonly used activation function in CNNs.\n",
        "\n",
        "### How CNNs Learn Spatial Hierarchies\n",
        "\n",
        "CNNs learn spatial hierarchies automatically. The initial layers might learn to detect edges, the next layers learn to detect shapes by combining edges, further layers might detect more complex structures. This ability to learn spatial hierarchies from raw data gives CNNs their power. It allows them to detect complex objects in images by combining simpler features from the earlier layers.\n",
        "\n",
        "### Advantages of CNNs\n",
        "\n",
        "- **Parameter Sharing**: A feature detector (filter) that's useful in one part of the image can be useful in another part of the image {cite}`krizhevsky2012imagenet`.\n",
        "- **Sparsity of Connections**: In each layer, each output value depends only on a small number of input values, making the computation more efficient.\n",
        "\n",
        "### Basic Code Implementation\n",
        "\n",
        "Below, you'll find a basic Convolutional Neural Network (CNN) structure implemented in TensorFlow. Treat this as a foundational blueprint for your subsequent implementations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6ovIjRxmvMy",
        "outputId": "88240d61-7b16-426a-bb41-599de269ffae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "476/476 [==============================] - 6s 5ms/step - loss: 0.6998 - accuracy: 0.6710 - val_loss: 0.4909 - val_accuracy: 0.8186\n",
            "Epoch 2/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.3956 - accuracy: 0.8329 - val_loss: 0.3862 - val_accuracy: 0.8327\n",
            "Epoch 3/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.3253 - accuracy: 0.8488 - val_loss: 0.3073 - val_accuracy: 0.8564\n",
            "Epoch 4/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.3007 - accuracy: 0.8616 - val_loss: 0.2985 - val_accuracy: 0.8706\n",
            "Epoch 5/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.2837 - accuracy: 0.8708 - val_loss: 0.2862 - val_accuracy: 0.8777\n",
            "Epoch 6/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.2767 - accuracy: 0.8786 - val_loss: 0.2732 - val_accuracy: 0.8712\n",
            "Epoch 7/10\n",
            "476/476 [==============================] - 2s 3ms/step - loss: 0.2578 - accuracy: 0.8876 - val_loss: 0.2548 - val_accuracy: 0.8995\n",
            "Epoch 8/10\n",
            "476/476 [==============================] - 2s 5ms/step - loss: 0.2437 - accuracy: 0.8984 - val_loss: 0.2720 - val_accuracy: 0.8759\n",
            "Epoch 9/10\n",
            "476/476 [==============================] - 2s 4ms/step - loss: 0.2351 - accuracy: 0.9041 - val_loss: 0.2287 - val_accuracy: 0.9078\n",
            "Epoch 10/10\n",
            "476/476 [==============================] - 2s 4ms/step - loss: 0.2289 - accuracy: 0.9099 - val_loss: 0.2185 - val_accuracy: 0.9161\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a18cef5fca0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "#model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(3, 3, 4), padding='SAME'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))# Add additional convolutional and pooling layers as needed\n",
        "# ...\n",
        "\n",
        "# Add dense layers for classification\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))  # 10 is the number of classes\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train[:,:,:,[2, 5, 7, 16]], y_train, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u7IH3IamHWl"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/Week_2/Model_CNN/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5UdQIRSz9n3"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Random Forest is a notable and significant part of machine learning and is commonly used for classification. It can also be used for regression, but its application in classification is more prevalent. Decision Trees are the core components of a Random Forest, so let's delve into the concepts of Decision Trees {cite}`breiman2001random,quinlan1986induction`.\n",
        "\n",
        "### Theoretical Foundations\n",
        "\n",
        "### 1. **Ensemble Learning**\n",
        "\n",
        "Ensemble methods employ multiple learning algorithms to achieve better predictive performance than any individual learning algorithm alone {cite}`dietterich2000ensemble`. The primary principle behind ensemble models is that several weak learners come together to form a strong learner.\n",
        "\n",
        "### 2. **Decision Trees**\n",
        "\n",
        "Decision trees are central to a Random Forest. They split data into subsets based on feature values, recursively producing a decision tree {cite}`quinlan1986induction`.\n",
        "\n",
        "### 3. **Bootstrap Aggregating (Bagging)**\n",
        "\n",
        "Random Forests leverage bagging, where multiple dataset subsets are created by drawing samples with replacement. A separate decision tree is built for each of these samples {cite}`breiman1996bagging`.\n",
        "\n",
        "### 4. **Feature Randomness**\n",
        "\n",
        "In conventional decision trees, the best feature is chosen to split data at every node. However, Random Forests introduce randomness by selecting a random set of features, then choosing the best split from this subset, ensuring a diverse ensemble of trees.\n",
        "\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Generalisation**: By combining the predictions of multiple trees, Random Forests tend to generalize better and are less susceptible to overfitting on training data.\n",
        "  \n",
        "- **Parallel Processing**: Each decision tree can be built independently, allowing for parallel processing which speeds up the algorithm considerably for large datasets.\n",
        "\n",
        "- **Handling Missing Values**: Random Forests can handle missing values and still produce reasonable predictions.\n",
        "\n",
        "- **Importance Scoring**: They provide an importance score for each feature, aiding in feature selection or interpretability.\n",
        "\n",
        "### Implementation in Python (Using Scikit-learn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHPPt3PDysZS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialise the model with n_estimators specifying the number of trees in the forest\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# We need to reshape the data in order to be compatible with Random Forest\n",
        "X_reshaped = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "# Fit the model to your training data\n",
        "clf.fit(X_reshaped, y_train)\n",
        "\n",
        "# Predict the classes of the test data\n",
        "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "y_pred = clf.predict(X_test_reshaped)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Week_2/Model_RF')"
      ],
      "metadata": {
        "id": "C1zpv2tGvt6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rlMqePQFLu"
      },
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "Vision Transformers (ViTs) are a recent breakthrough in the field of deep learning for image processing. They depart from the traditional convolutional neural network (CNN) approach and apply transformers, which were originally designed for natural language processing tasks, to image classification.\n",
        "\n",
        "### Theoretical Foundations\n",
        "\n",
        "### 1. **Tokenisation of Images**\n",
        "\n",
        "Instead of processing images using convolutions, ViTs divide the image into fixed-size patches, linearly embed them, and then process the resulting sequence of vectors (or tokens) using a transformer.{cite}`dosovitskiy2020image`\n",
        "\n",
        "### 2. **Position Embeddings**\n",
        "\n",
        "Since the original transformer doesn't have a notion of the relative positions of tokens, positional embeddings are added to the patch embeddings to retain the positional information.{cite}`dosovitskiy2020image`\n",
        "\n",
        "### 3. **Transformer Architecture**\n",
        "\n",
        "The core of ViT is the transformer architecture, which consists of multiple layers of multi-head self-attention mechanisms and feed-forward neural networks.{cite}`dosovitskiy2020image`\n",
        "\n",
        "### 4. **Classification Head**\n",
        "\n",
        "After processing through the transformer layers, the embedding of the first token (often referred to as the 'CLS' token) is used to classify the image.{cite}`dosovitskiy2020image`\n",
        "\n",
        "### Advantages of ViT\n",
        "\n",
        "- **Model Transferability**: ViTs pre-trained on large datasets can be fine-tuned on smaller datasets, achieving high performance even when the available labeled data is limited.\n",
        "\n",
        "- **Scalability**: ViTs are more data-hungry compared to CNNs. However, their performance continues to improve as the model size and the amount of data increase, often surpassing other architectures.\n",
        "\n",
        "- **Flexibility**: The transformer architecture isn't specialized for grid-like data (like images), making ViTs potentially more flexible for varied input data types.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- **Computational Demand**: ViTs can be computationally intensive, especially when dealing with large images or when the model has many layers.\n",
        "\n",
        "- **Data Requirement**: To achieve optimal performance, ViTs often require more training data compared to CNNs.\n",
        "\n",
        "### Implementation\n",
        "The implmentation of Vision Transformer is much more complicated than CNN and Random Forest as there is no built-in functions or layers in the library. However, the following code uses some existing functions like Muliti-head attention to build the transformer block. You don't need to know the exactly and detailed structure of ViT as it is not required in this course. Please follow the code below for example of implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5n6OIjeQKTq",
        "outputId": "93786141-a3db-4938-a360-d2b81edf4e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/611.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "# Install this additional Tensorflow package for ViT\n",
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4dDNKfHwF4t",
        "outputId": "f0371981-aba7-42c7-a60f-13ea770b4001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Install packages needed\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7yFFC8HwF4t"
      },
      "outputs": [],
      "source": [
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = more_data(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=X_train,\n",
        "        y=y_train,\n",
        "        batch_size=30,\n",
        "        epochs=20,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt7nBLb8wF4u"
      },
      "outputs": [],
      "source": [
        "num_classes = 2 #Can be changed to multi-classed classification\n",
        "input_shape = (3, 3, 21)#depends on the size of the image we want\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72\n",
        "patch_size = 6\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Mx50rnwF4u"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "more_data = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"more_data\",\n",
        ")\n",
        "more_data.layers[0].adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMJl3ZfwF4v",
        "outputId": "2ca3ad70-019c-49bc-bdc6-51bb128d30ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "508/508 [==============================] - 63s 97ms/step - loss: 0.5881 - accuracy: 0.9005 - top-5-accuracy: 1.0000 - val_loss: 0.1800 - val_accuracy: 0.9226 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "508/508 [==============================] - 37s 72ms/step - loss: 0.1970 - accuracy: 0.9288 - top-5-accuracy: 1.0000 - val_loss: 0.1703 - val_accuracy: 0.9344 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "508/508 [==============================] - 36s 71ms/step - loss: 0.1873 - accuracy: 0.9337 - top-5-accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9368 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.1855 - accuracy: 0.9332 - top-5-accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9403 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "508/508 [==============================] - 36s 70ms/step - loss: 0.1898 - accuracy: 0.9338 - top-5-accuracy: 1.0000 - val_loss: 0.1802 - val_accuracy: 0.9409 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "508/508 [==============================] - 36s 71ms/step - loss: 0.1741 - accuracy: 0.9375 - top-5-accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9492 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "508/508 [==============================] - 35s 68ms/step - loss: 0.1738 - accuracy: 0.9368 - top-5-accuracy: 1.0000 - val_loss: 0.1630 - val_accuracy: 0.9421 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "508/508 [==============================] - 35s 68ms/step - loss: 0.1686 - accuracy: 0.9394 - top-5-accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9344 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "508/508 [==============================] - 35s 68ms/step - loss: 0.1668 - accuracy: 0.9389 - top-5-accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9433 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.1561 - accuracy: 0.9417 - top-5-accuracy: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.9492 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "508/508 [==============================] - 36s 70ms/step - loss: 0.1554 - accuracy: 0.9427 - top-5-accuracy: 1.0000 - val_loss: 0.1220 - val_accuracy: 0.9509 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.1581 - accuracy: 0.9411 - top-5-accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9474 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.1532 - accuracy: 0.9445 - top-5-accuracy: 1.0000 - val_loss: 0.1238 - val_accuracy: 0.9486 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "508/508 [==============================] - 35s 68ms/step - loss: 0.1474 - accuracy: 0.9442 - top-5-accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.1515 - accuracy: 0.9449 - top-5-accuracy: 1.0000 - val_loss: 0.1250 - val_accuracy: 0.9504 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "508/508 [==============================] - 34s 68ms/step - loss: 0.1469 - accuracy: 0.9470 - top-5-accuracy: 1.0000 - val_loss: 0.1333 - val_accuracy: 0.9504 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.1453 - accuracy: 0.9480 - top-5-accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9415 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.1457 - accuracy: 0.9466 - top-5-accuracy: 1.0000 - val_loss: 0.1737 - val_accuracy: 0.9444 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.1409 - accuracy: 0.9467 - top-5-accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9468 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.1453 - accuracy: 0.9453 - top-5-accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9504 - val_top-5-accuracy: 1.0000\n",
            "59/59 [==============================] - 1s 22ms/step - loss: 0.1162 - accuracy: 0.9543 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 95.43%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Here vit is your trained model after the training\n",
        "vit = create_vit_classifier()\n",
        "history = run_experiment(vit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Week_2/Model_ViT')"
      ],
      "metadata": {
        "id": "PjsdSD59yrjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_dT4T61wF4v"
      },
      "source": [
        "## Model Selection and Cross-validation\n",
        "Cross-validation and model selection are pivotal components in ensuring the robustness of machine learning models. They ensure that our model doesn't just memorise the training data (overfitting) and that it generalises well to new, unseen data. However, they are some small nuances between this procedure in Deep Learning models (like CNN and ViT) and traditional Machine Learning models (Random Forests)\n",
        "\n",
        "### Deep Learning Models:\n",
        "\n",
        "- **Validation During Training**:\n",
        "    - Deep learning models, particularly when trained on large datasets, often incorporate a validation set during the training process. This is done to monitor the model's generalization performance and to use mechanisms like early stopping, learning rate annealing based on validation loss, etc.\n",
        "\n",
        "- **Cross-Validation Less Common**:\n",
        "    - Due to the computational intensity and time required to train deep learning models, k-fold cross-validation is less commonly used. Instead, a single hold-out validation set (or sometimes a few different validation sets) is used.\n",
        "\n",
        "- **Model Selection**:\n",
        "    - While the principles of model selection apply to deep learning, the specific approach might be different. Hyperparameter tuning in deep learning might involve methods like random search, Bayesian optimization, or dedicated libraries like Optuna or Ray Tune, instead of just grid search.\n",
        "\n",
        "### Traditional ML Models:\n",
        "\n",
        "- **Cross-Validation**:\n",
        "    - For many traditional machine learning models, k-fold cross-validation is a standard technique because these models are typically faster to train. Cross-validation gives a more robust estimate of the model's performance.\n",
        "\n",
        "- **Model Selection**:\n",
        "    - Grid search combined with cross-validation (e.g., `GridSearchCV` in scikit-learn) is a common method for hyperparameter tuning and model selection for traditional algorithms.\n",
        "\n",
        "### Overlap & Best Practices:\n",
        "\n",
        "- Despite these general trends, it's worth noting that there's overlap. Deep learning models can also be evaluated using cross-validation if computational resources permit. Similarly, traditional ML models can (and do) use validation sets during training, especially for iterative algorithms like gradient boosting machines.\n",
        "\n",
        "- The choice between using a validation set during training or relying on cross-validation often depends on the dataset's size, computational resources, and specific project requirements.\n",
        "\n",
        "- Actually we have included validation procedure in the building of CNN and ViT. Check for code that contains 'validation or validation split'.\n",
        "\n",
        "\n",
        "### What is Cross-Validation? {cite}`bishop2006pattern`\n",
        "\n",
        "Cross-validation is a technique to evaluate the performance of a model by splitting the dataset into a training set and a validation set multiple times. The most common method is k-fold cross-validation.\n",
        "### K-Fold Cross-Validation {cite}`bishop2006pattern`\n",
        "\n",
        "In k-fold cross-validation, the training data is randomly partitioned into k equal-sized subsets. Of the k subsets, a single subset is retained as validation data, and the remaining k-1 subsets are used as training data. The cross-validation process is repeated k times, with each of the k subsets used exactly once as validation data. The k results can then be averaged to produce a single estimation.\n",
        "\n",
        "The following code is a example of K-Fold CV for Random Forest model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itYHUBIewF4v",
        "outputId": "17c6dc50-913e-4ebb-d1de-fc05e3a07eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean cross-validation score:  0.959850929605414\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# Assuming `model` is your sklearn model and `X` and `y` are your data\n",
        "# Perform 5-fold cross-validation\n",
        "scores = cross_val_score(clf, X_reshaped, y_train, cv=5)\n",
        "\n",
        "# Print the mean of the cross-validation scores\n",
        "print(\"Mean cross-validation score: \", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6U2iT8GwF4v"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Model selection involves choosing the best model from a set of models based on performance. The model with the best cross-validation score is typically selected.\n",
        "\n",
        "### Grid Search\n",
        "\n",
        "A popular technique for model selection is grid search. Grid search involves:\n",
        "\n",
        "- Specifying a subset of the hyperparameter space.\n",
        "- Training a model for each hyperparameter combination.\n",
        "- Evaluating each model using cross-validation.\n",
        "- Selecting the model with the best cross-validation performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIjMIuJMwF4w",
        "outputId": "6eee6054-2ad6-41af-f8a7-a823c086a393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:  {'max_depth': 20, 'n_estimators': 150}\n",
            "Best cross-validation score:  0.9606787022390625\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example for a hypothetical model's hyperparameters\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_reshaped, y_train)\n",
        "\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}